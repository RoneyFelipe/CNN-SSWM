{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import geopandas as gpd\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from shapely import Polygon\n",
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import mmcv\n",
    "from mmseg.apis import MMSegInferencer\n",
    "import shapely\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy2(transform):\n",
    "    return np.array([transform.a,\n",
    "                     transform.b,\n",
    "                     transform.c,\n",
    "                     transform.d,\n",
    "                     transform.e,\n",
    "                     transform.f, 0, 0, 1], dtype='float64').reshape((3, 3))\n",
    "\n",
    "\n",
    "def xy_np(transform, rows, cols, min_x, min_y, offset='center'):\n",
    "    if isinstance(rows, int) and isinstance(cols, int):\n",
    "        pts = np.array([[rows+min_y, cols+min_x, 1]]).T\n",
    "    else:\n",
    "        assert len(rows) == len(cols)\n",
    "        pts = np.ones((3, len(rows)), dtype=int)\n",
    "        pts[0] = rows + min_y\n",
    "        pts[1] = cols + min_x\n",
    "    if offset == 'center':\n",
    "        coff, roff = (0.5, 0.5)\n",
    "    elif offset == 'ul':\n",
    "        coff, roff = (0, 0)\n",
    "    elif offset == 'ur':\n",
    "        coff, roff = (1, 0)\n",
    "    elif offset == 'll':\n",
    "        coff, roff = (0, 1)\n",
    "    elif offset == 'lr':\n",
    "        coff, roff = (1, 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid offset\")\n",
    "    _transnp = to_numpy2(transform)\n",
    "    _translt = to_numpy2(transform.translation(coff, roff))\n",
    "    locs = _transnp @ _translt @ pts\n",
    "    lat, lon = locs[0].tolist(), locs[1].tolist()\n",
    "    coords = [(lat[i], lon[i]) for i in range(len(lat))]\n",
    "    return coords\n",
    "\n",
    "def polygons_from_binary_image(img, transform, crs, min_x=0, min_y=0, min_area=5):\n",
    "\n",
    "    assert isinstance(img, np.ndarray), 'img deve ser um numpy array'\n",
    "\n",
    "    unique_values = np.unique(img)\n",
    "\n",
    "    new_geo_data_frame = {\"geometry\": [], 'CLASSE': []}\n",
    "\n",
    "    for cat in unique_values:\n",
    "        if cat == 0:\n",
    "            continue\n",
    "\n",
    "        img_ = img.copy()\n",
    "        img_[img_ != cat] = 0\n",
    "        img_[img_ != 0] = 1\n",
    "\n",
    "        contours, hierarchy = cv2.findContours(img_, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        categ = cat\n",
    "\n",
    "        interiors = [xy_np(transform, contour[:, 0, 0], contour[:, 0, 1], min_x, min_y)\n",
    "                     for c, contour in enumerate(contours) if hierarchy[0][c][3] != -1]\n",
    "        index = [\n",
    "            hierarchy[0][c][3]\n",
    "            for c, contour in enumerate(contours)\n",
    "            if hierarchy[0][c][3] != -1\n",
    "        ]\n",
    "\n",
    "        for c, contour in tqdm(enumerate(contours)):\n",
    "            if hierarchy[0][c][3] == -1:\n",
    "                if cv2.contourArea(contour) < min_area:\n",
    "                    continue\n",
    "                exterior = xy_np(\n",
    "                    transform, contour[:, 0, 0], contour[:, 0, 1], min_x, min_y)\n",
    "                interior = [hole for h, hole in enumerate(\n",
    "                    interiors) if index[h] == c]\n",
    "                if len(exterior) <= 3:\n",
    "                    continue\n",
    "                poly = shapely.geometry.polygon.Polygon(exterior, interior)\n",
    "                new_geo_data_frame[\"geometry\"].append(poly)\n",
    "                new_geo_data_frame[\"CLASSE\"].append(categ)\n",
    "\n",
    "    gdf1 = gpd.GeoDataFrame.from_dict(new_geo_data_frame, geometry=\"geometry\", crs=crs)\n",
    "\n",
    "    return gdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config_file, checkpoint_file, device):\n",
    "    # build the model from a config file and a checkpoint file\n",
    "    model = init_model(config_file, checkpoint_file, device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(path):\n",
    "    filenames = [f for f in os.listdir(path) if f.endswith('.tif')]\n",
    "    tif_path = os.path.join(path, filenames[0]) if len(filenames) > 0 else None\n",
    "\n",
    "    filenames = [f for f in os.listdir(path) if f.endswith('.shp')]\n",
    "    shp_path = os.path.join(path, filenames[0]) if len(filenames) > 0 else None\n",
    "\n",
    "    return tif_path, shp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../../modelo_mamona/iter_20000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipe-amarelo/CNN-SSWM/mmsegmentation-main/mmseg/models/decode_heads/decode_head.py:120: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert `seg_logits` into a predictionapplying a threshold\n",
      "  warnings.warn('For binary segmentation, we suggest using'\n",
      "/home/ipe-amarelo/CNN-SSWM/mmsegmentation-main/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config_file = '../../modelo_mamona/segformer_mit-b0_8xb2-160k_ade20k-512x512_mamona.py'\n",
    "checkpoint_file = '../../modelo_mamona/iter_20000.pth'\n",
    "\n",
    "model = get_model(config_file, checkpoint_file, 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Ortos as serem processados:  1\n",
      "Ortos a serem processados:\n",
      "['110158_SANTO ANTONIO_01']\n"
     ]
    }
   ],
   "source": [
    "patch_size= 256\n",
    "step = patch_size // 2\n",
    "\n",
    "path_folder = '../../tifs/'\n",
    "orto_files = os.listdir(path_folder)\n",
    "    \n",
    "cont = 1\n",
    "\n",
    "print(\"Total de Ortos as serem processados: \", len(orto_files))\n",
    "print(\"Ortos a serem processados:\")\n",
    "\n",
    "print(orto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Iniciando processamento ---------------------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------- Iniciando processamento ---------------------------- \")\n",
    "\n",
    "for file in tqdm(orto_files):\n",
    "    # Caminho da ortofoto\n",
    "    path_orto = os.path.join(os.path.join(path_folder, file))\n",
    "    filename_orto = os.path.basename(tif_path)[:-4]\n",
    "    tif_path, shp_path = get_paths(path_orto)\n",
    "    \n",
    "    if tif_path is None:\n",
    "        print(f'Nenhum arquivo .tif encontrado em {tif_path}')\n",
    "        continue\n",
    "\n",
    "    if shp_path is None:\n",
    "        print(f'Nenhum arquivo .shp encontrado em {shp_path}')\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(geo_data_frame, func_latlon_xy, index, min_size):\n",
    "    geo_data_frame = geo_data_frame.explode(ignore_index=True)\n",
    "\n",
    "    bounds = geo_data_frame.iloc[[index]].bounds\n",
    "    min_lat = bounds[\"minx\"].min()\n",
    "    max_lon = bounds[\"miny\"].min()\n",
    "    max_lat = bounds[\"maxx\"].max()\n",
    "    min_lon = bounds[\"maxy\"].max()\n",
    "\n",
    "    min_x, min_y = func_latlon_xy(min_lat, min_lon)\n",
    "    max_x, max_y = func_latlon_xy(max_lat, max_lon)\n",
    "\n",
    "    # assert max_x-min_x > 0 and max_y-min_y > 0, 'Erro de dimensoes'\n",
    "    if not ((max_x - min_x > 0) and (max_y - min_y > 0)):\n",
    "        print(\"Talhão vazio!\")\n",
    "        return None\n",
    "\n",
    "    if max_x - min_x < min_size:\n",
    "        max_x = min_x + min_size\n",
    "    if max_y - min_y < min_size:\n",
    "        max_y = min_y + min_size\n",
    "\n",
    "    min_x, min_y, max_x, max_y = int(min_x), int(min_y), int(max_x), int(max_y)\n",
    "    width, height = int(max_x - min_x), int(max_y - min_y)\n",
    "\n",
    "    # Criar uma imagem em branco usando OpenCV\n",
    "    image = np.zeros((width, height), dtype=np.uint8)\n",
    "\n",
    "    geometry = geo_data_frame.geometry.iloc[index]\n",
    "\n",
    "\n",
    "    pts = np.array(\n",
    "        [\n",
    "            func_latlon_xy(point[0], point[1])\n",
    "            for point in geometry.exterior.coords[:]\n",
    "        ],\n",
    "        np.int32,\n",
    "    )[:, ::-1]\n",
    "    pts = pts - np.array([min_y, min_x])\n",
    "            \n",
    "    cv2.fillPoly(image, [pts], 1)\n",
    "    for interior in geometry.interiors:\n",
    "        pts = np.array(\n",
    "            [\n",
    "                func_latlon_xy(point[0], point[1])\n",
    "                for point in interior.coords[:]\n",
    "            ],\n",
    "            np.int32,\n",
    "        )[:, ::-1]\n",
    "        pts = pts - np.array([min_y, min_x])\n",
    "        cv2.fillPoly(image, [pts], 0)\n",
    "\n",
    "    return (\n",
    "        image,\n",
    "        (min_x, min_y, max_x, max_y),\n",
    "        (min_lat, max_lat, min_lon, max_lon),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_patch(dataset, x, y, w, h):\n",
    "    nc = 3\n",
    "    img = np.zeros((h, w, nc), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(nc):\n",
    "        band = dataset.read(i+1, window=Window(y, x, w, h))\n",
    "        nw, nh = band.shape[0], band.shape[1]\n",
    "        if nw == 0 or nh == 0:\n",
    "            return None\n",
    "        img[0:nw, 0:nh, i] = band\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_talhoes = gpd.read_file(shp_path)\n",
    "dataset = rasterio.open(tif_path)\n",
    "gpd_talhoes = gpd_talhoes.to_crs(dataset.crs)\n",
    "shp_all_talhoes = []\n",
    "\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, (min_x, min_y, max_x, max_y), (min_lat, max_lat, min_lon, max_lon) = get_img(gpd_talhoes, dataset.index, index=t, min_size=256)\n",
    "min_x, min_y, max_x, max_y = int(min_x), int(min_y), int(max_x), int(max_y)\n",
    "width, height = int(max_x-min_x), int(max_y-min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [01:29<00:00,  1.92it/s]\n",
      "261it [00:00, 17476.55it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "imgs = []\n",
    "positions = []\n",
    "results_probs = np.zeros((2, width, height))\n",
    "cont = 0\n",
    "\n",
    "for x in tqdm(range(min_x, max_x-1, step)):\n",
    "    discard_x1, discard_x2 = 0.1, 0.9\n",
    "    if x == min_x:\n",
    "        discard_x1 = 0.\n",
    "    if x+patch_size >= max_x:\n",
    "        x = max_x-patch_size\n",
    "        discard_x2 = 1.\n",
    "    for y in range(min_y, max_y-1, step):\n",
    "        discard_y1, discard_y2 = 0.1, 0.9\n",
    "        if y+patch_size >= max_y:\n",
    "            discard_y2 = 1.\n",
    "            y = max_y-patch_size\n",
    "        if y == min_y:\n",
    "            discard_y1 = 0.\n",
    "\n",
    "        x1 = (x-min_x)+int(patch_size*discard_x1)\n",
    "        y1 = (y-min_y)+int(patch_size*discard_y1)\n",
    "        x2 = (x-min_x)+int(patch_size*discard_x2)\n",
    "        y2 = (y-min_y)+int(patch_size*discard_y2)\n",
    "\n",
    "        mask_patch = mask[x1:x2, y1:y2]\n",
    "        if np.sum(mask_patch) == 0:\n",
    "            continue\n",
    "\n",
    "        img = get_image_patch(dataset, x, y, patch_size, patch_size)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = img[:, :, [2, 1, 0]]\n",
    "        imgs.append(img)\n",
    "        positions.append([x1, x2, y1, y2, discard_x1, discard_x2, discard_y1, discard_y2, x, y])\n",
    "\n",
    "        if len(imgs) >= batch_size:\n",
    "            results_all = inference_model(model, imgs)\n",
    "\n",
    "            for i in range(len(results_all)):\n",
    "                #Image.fromarray(imgs[i]).save(f'img_{cont}.png')\n",
    "                x1, x2, y1, y2, discard_x1, discard_x2, discard_y1, discard_y2, x, y = positions[i]\n",
    "\n",
    "                patch_daninha = F.softmax(results_all[i].seg_logits.data, dim=0).cpu().numpy()\n",
    "\n",
    "                #Image.fromarray(np.argmax(patch_daninha, axis=0).astype(np.uint8)*255).save(f'pred_{cont}.png')\n",
    "                cont += 1\n",
    "\n",
    "                patch_daninha = patch_daninha[:, int(patch_size*discard_x1):int(patch_size*discard_x2), int(patch_size*discard_y1):int(patch_size*discard_y2)]\n",
    "                results_probs[:, x1:x2, y1:y2] = results_probs[:, x1:x2, y1:y2] + patch_daninha\n",
    "            imgs = []\n",
    "            positions = []\n",
    "\n",
    "if len(imgs) > 0:\n",
    "    results_all = inference_model(model, imgs)\n",
    "\n",
    "    for i in range(len(results_all)):\n",
    "        #Image.fromarray(imgs[i]).save(f'img_{i}.png')\n",
    "        x1, x2, y1, y2, discard_x1, discard_x2, discard_y1, discard_y2, x, y = positions[i]\n",
    "\n",
    "        patch_daninha = F.softmax(results_all[i].seg_logits.data, dim=0).cpu().numpy()\n",
    "        patch_daninha = patch_daninha[:, int(patch_size*discard_x1):int(patch_size*discard_x2), int(patch_size*discard_y1):int(patch_size*discard_y2)]\n",
    "\n",
    "        results_probs[:, x1:x2, y1:y2] = results_probs[:, x1:x2, y1:y2] + patch_daninha\n",
    "\n",
    "results_daninha = np.argmax(results_probs, axis=0).astype(np.uint8)   \n",
    "if mask is not None:\n",
    "    results_daninha[mask == 0] = 0\n",
    "\n",
    "prediction_shp = polygons_from_binary_image(results_daninha, dataset.transform, dataset.crs, min_x=min_x, min_y=min_y)\n",
    "prediction_shp.to_file(os.path.join(path_orto, f'./mamona_{filename_orto}.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "462it [00:00, 4840.57it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
